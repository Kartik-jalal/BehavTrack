{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(source_dir, metadata_filename):\n",
    "    metadata_filePath = os.path.join(source_dir, metadata_filename)\n",
    "\n",
    "    with open(metadata_filePath, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(output_dir, metadata_filename, metadata):\n",
    "    metadata_outFilePath = os.path.join(output_dir, metadata_filename)\n",
    "\n",
    "    with open(metadata_outFilePath, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSplit(metadata, split_ratio):\n",
    "    clusters = [entry['cluster'] for entry in metadata]\n",
    "    \n",
    "    activeLearning_data, prediction_data = train_test_split(\n",
    "        metadata,\n",
    "        test_size=split_ratio,\n",
    "        random_state=42,\n",
    "        stratify=clusters  # stratify \n",
    "        )\n",
    "    \n",
    "    return activeLearning_data, prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyframes(split):\n",
    "    # destination path for the split frame\n",
    "    out_path = os.path.join(split[2][1], split[0][0])\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    # save the metadata at the output\n",
    "    save_metadata(split[2][1], split[0][1], split[1][0])\n",
    "\n",
    "    if split[1][1]:\n",
    "        for data in split[1][0]:\n",
    "            frame_name = os.path.basename(data['image_path'])\n",
    "            # src frame path\n",
    "            src_framePath = os.path.join(split[2][0], frame_name)\n",
    "            out_framePath = os.path.join(out_path, frame_name)\n",
    "            # copy\n",
    "            shutil.copy2(src_framePath, out_framePath)\n",
    "\n",
    "    print(f\"Copy complete for the split {split[0][1]} with images - {split[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCluster_ratio(data):\n",
    "    # Count the number of frames per cluster\n",
    "    cluster_counts = Counter(entry['cluster'] for entry in data)\n",
    "    # Calculate total number of frames\n",
    "    total_frames = len(data)\n",
    "    # Print cluster percentages\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        percentage = (count / total_frames) * 100\n",
    "        print(f\"Cluster {cluster_id}: {count} frames ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDetails(metadata, split_data):\n",
    "    print(\"#############\")\n",
    "    print(f\"Total frames: {len(metadata)}\")\n",
    "    getCluster_ratio(metadata)\n",
    "    print(\"\\t#############\")\n",
    "    for split in split_data:\n",
    "        percentage = (len(split[1][0])/len(metadata))*100\n",
    "        print(f\"{split[0][1]} split: {len(split[1][0])} ({percentage:.2f}%)\")\n",
    "        getCluster_ratio(split[1][0])\n",
    "        print(\"\\t#############\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyframes_train_val(currentFrame_path, destination_dir):\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    shutil.copy2(currentFrame_path, destination_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        return img.width, img.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_bBox_labels(annotation_bBox_info, frame_w, frame_h, class_label):\n",
    "    yolo_bBox_label = []\n",
    "    for bBox_info in annotation_bBox_info:\n",
    "        bBox = bBox_info['bbox']\n",
    "\n",
    "        x1, y1 = bBox[\"x1\"], bBox[\"y1\"]\n",
    "        x2, y2 = bBox[\"x2\"], bBox[\"y2\"]\n",
    "\n",
    "        # Convert to YOLO\n",
    "        bBox_w = x2 - x1\n",
    "        bBox_h = y2 - y1\n",
    "        x_center = x1 + bBox_w / 2.0\n",
    "        y_center = y1 + bBox_h / 2.0\n",
    "        \n",
    "        # Normalize\n",
    "        x_center_norm = x_center / frame_w\n",
    "        y_center_norm = y_center / frame_h\n",
    "        w_norm = bBox_w / frame_w\n",
    "        h_norm = bBox_h / frame_h\n",
    "        \n",
    "        # class_id, x_c, y_c, w, h\n",
    "        yolo_line = f\"{class_label} {x_center_norm:.6f} {y_center_norm:.6f} {w_norm:.6f} {h_norm:.6f}\"\n",
    "        \n",
    "        for keypoint in bBox_info['keypoints'].values():\n",
    "            kp_x = keypoint[0]/frame_w\n",
    "            kp_y = keypoint[1]/frame_h\n",
    "            kp_v = keypoint[2]\n",
    "\n",
    "            keypoint_line = f\" {kp_x:.6f} {kp_y:.6f} {kp_v}\"\n",
    "\n",
    "            yolo_line += keypoint_line\n",
    "        \n",
    "        yolo_bBox_label.append(yolo_line)\n",
    "        \n",
    "    return yolo_bBox_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yolo_label(trainLabels_dir, frame_name, yolo_bBox_labels):\n",
    "    os.makedirs(trainLabels_dir, exist_ok=True)\n",
    "    trainLabel_filename = os.path.join(trainLabels_dir, frame_name.replace(\"jpg\", \"txt\"))\n",
    "\n",
    "    with open(trainLabel_filename, 'w') as txt_out:\n",
    "        txt_out.write(\"\\n\".join(yolo_bBox_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_val(t_v_dir, annotations_dir, frame_name, annotation_info):\n",
    "    t_v_images_dir = os.path.join(t_v_dir, \"images\")\n",
    "    currentFrame_path = os.path.join(annotations_dir, frame_name)\n",
    "    copyframes_train_val(currentFrame_path, t_v_images_dir)\n",
    "\n",
    "    frame_w, frame_h = get_image_size(currentFrame_path)\n",
    "    mouse_class_label = 0\n",
    "    t_v_labels_dir = os.path.join(t_v_dir, \"labels\")\n",
    "    yolo_bBox_labels = create_yolo_bBox_labels(annotation_info,  frame_w, frame_h, mouse_class_label)\n",
    "    save_yolo_label(t_v_labels_dir, frame_name, yolo_bBox_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_txt_to_annotation_json(\n",
    "    txt_path, \n",
    "    image_filename,   # \"image_filename.jpg\"\n",
    "    image_width, \n",
    "    image_height,\n",
    "    keypoint_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a YOLO-like .txt (with bbox + 4 keypoints in normalized coords),\n",
    "    and returns a dictionary in the original annotation style:\n",
    "\n",
    "    {\n",
    "      \"image_filename\": [\n",
    "        {\n",
    "          \"bbox\": {\"x1\":..., \"y1\":..., \"x2\":..., \"y2\":...},\n",
    "          \"keypoints\": {\n",
    "            \"nose\":  [...],\n",
    "            \"earL\":  [...],\n",
    "            \"earR\":  [...],\n",
    "            \"tailB\": [...]\n",
    "          }\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    if keypoint_names is None:\n",
    "        # You can change the order or number of keypoints as needed:\n",
    "        keypoint_names = [\"nose\", \"earL\", \"earR\", \"tailB\"]\n",
    "\n",
    "    annotations = {image_filename: []}\n",
    "\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        tokens = line.split()\n",
    "        # The first 5 tokens are class_id, x_center, y_center, w, h\n",
    "        class_id    = int(tokens[0])\n",
    "        x_center_n  = float(tokens[1])\n",
    "        y_center_n  = float(tokens[2])\n",
    "        w_n         = float(tokens[3])\n",
    "        h_n         = float(tokens[4])\n",
    "\n",
    "        # Denormalize bounding box\n",
    "        x_center = x_center_n * image_width\n",
    "        y_center = y_center_n * image_height\n",
    "        w        = w_n * image_width\n",
    "        h        = h_n * image_height\n",
    "\n",
    "        x1 = x_center - w / 2\n",
    "        y1 = y_center - h / 2\n",
    "        x2 = x_center + w / 2\n",
    "        y2 = y_center + h / 2\n",
    "\n",
    "        # Next tokens: each keypoint has x_kpt_n, y_kpt_n, v_kpt\n",
    "        # For 4 keypoints, that's 12 tokens, starting at index = 5\n",
    "        keypoints_dict = {}\n",
    "        num_kpts = len(keypoint_names)\n",
    "        \n",
    "        # i.e. for 4 keypoints, range(4) => 0..3\n",
    "        for i in range(num_kpts):\n",
    "            x_kpt_n = float(tokens[5 + 3*i])\n",
    "            y_kpt_n = float(tokens[5 + 3*i + 1])\n",
    "            v_kpt   = float(tokens[5 + 3*i + 2])\n",
    "\n",
    "            # denormalize\n",
    "            x_kpt = x_kpt_n * image_width\n",
    "            y_kpt = y_kpt_n * image_height\n",
    "\n",
    "            kpt_name = keypoint_names[i]\n",
    "            keypoints_dict[kpt_name] = [x_kpt, y_kpt, 2 if v_kpt > 0.5 else 1]\n",
    "\n",
    "        annotations[image_filename].append({\n",
    "            \"bbox\": {\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2\n",
    "            },\n",
    "            \"keypoints\": keypoints_dict\n",
    "        })\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs' path\n",
    "source_dir = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/frames\"\n",
    "output_dir = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/stratifySplit_frames\"\n",
    "metadata_filename_main = \"frames_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the frames metadata\n",
    "metadata_main = load_metadata(source_dir, metadata_filename_main)\n",
    "\n",
    "# Initial Split Percentages [active learning - 85, test - 15]\n",
    "split_ratio_main = 0.15\n",
    "\n",
    "# perform intial split\n",
    "activeLearning_data, test_data = performSplit(metadata_main, split_ratio_main)\n",
    "split_data_main = [(\n",
    "        ('activeLearning', 'activeLearning.json'),\n",
    "        (activeLearning_data, False),\n",
    "        (source_dir, output_dir)\n",
    "    ),\n",
    "    (\n",
    "        ('test', 'test.json'),\n",
    "        (test_data, True),\n",
    "        (source_dir, output_dir)\n",
    "    )]\n",
    "\n",
    "printDetails(metadata_main, split_data_main)\n",
    "\n",
    "# copy the frames to their appropriate dirs\n",
    "for split in split_data_main:\n",
    "    copyframes(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activeLearning_dir = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/stratifySplit_frames/activeLearning\"\n",
    "metadata_filename_al = \"activeLearning.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of initial frames of manuel annotation\n",
    "intialN_annotatonData = 100\n",
    "\n",
    "# load the active learning frames metadata\n",
    "activeLearning_metadata = load_metadata(activeLearning_dir, metadata_filename_al)\n",
    "\n",
    "# calculate the split ratio respective to intialN_annotatonData\n",
    "split_ratio_aL = round(((intialN_annotatonData/len(activeLearning_metadata))*100)/100, 3)\n",
    "\n",
    "# annotation split\n",
    "activeLearning_metadata_new, annotation_metadata = performSplit(activeLearning_metadata, split_ratio_aL)\n",
    "\n",
    "split_data_aL = (\n",
    "        ('activeLearning', 'activeLearning.json'),\n",
    "        (activeLearning_metadata_new, False),\n",
    "        (source_dir, output_dir)\n",
    "    )\n",
    "    \n",
    "split_data_annotations =   (\n",
    "        ('annotations', 'annotations_metadataAL.json'),\n",
    "        (annotation_metadata, True),\n",
    "        (source_dir, activeLearning_dir)\n",
    "    )\n",
    "\n",
    "printDetails(activeLearning_metadata, [split_data_aL, split_data_annotations])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_percentage = 30\n",
    "nOf_annotations = float((30*len(annotation_metadata))/100)\n",
    "split_ratio_predict = round(((nOf_annotations/len(activeLearning_metadata_new))*100)/100, 3)\n",
    "\n",
    "# annotation split\n",
    "activeLearning_metadata_updated, predict_metadata = performSplit(activeLearning_metadata_new, split_ratio_predict)\n",
    "\n",
    "split_data_aL = (\n",
    "        ('activeLearning', 'activeLearning.json'),\n",
    "        (activeLearning_metadata_updated, False),\n",
    "        (source_dir, output_dir)\n",
    "    )\n",
    "    \n",
    "split_data_predict =   (\n",
    "        ('predict', 'predict.json'),\n",
    "        (predict_metadata, True),\n",
    "        (source_dir, activeLearning_dir)\n",
    "    )\n",
    "\n",
    "printDetails(activeLearning_metadata_new, [split_data_aL, split_data_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_annotation = 0.15\n",
    "\n",
    "# train val split\n",
    "train_metadata, val_metadata = performSplit(annotation_metadata, split_ratio_annotation)\n",
    "\n",
    "split_data_train = (\n",
    "        ('train', 'train.json'),\n",
    "        (train_metadata, False),\n",
    "        (source_dir, activeLearning_dir)\n",
    "    )\n",
    "    \n",
    "split_data_val =   (\n",
    "        ('val', 'val.json'),\n",
    "        (val_metadata, False),\n",
    "        (source_dir, activeLearning_dir)\n",
    "    )\n",
    "\n",
    "printDetails(annotation_metadata, [split_data_train, split_data_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_is = [split_data_aL, split_data_annotations, split_data_train, split_data_val, split_data_predict]\n",
    "\n",
    "for split in split_data_is:\n",
    "    copyframes(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion\n",
    "Annotations to train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(activeLearning_dir, \"train\")\n",
    "train_json = \"train.json\"\n",
    "val_dir = os.path.join(activeLearning_dir, \"val\")\n",
    "val_json = \"val.json\"\n",
    "\n",
    "# path to manual annotated frames and its json\n",
    "annotations_dir = os.path.join(activeLearning_dir, \"annotations\")\n",
    "annotation_json = \"annotation.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually annotated json\n",
    "mAnnotated_json = load_metadata(annotations_dir, annotation_json)\n",
    "\n",
    "# train and val frame metadata\n",
    "train_metadata = load_metadata(train_dir, train_json)\n",
    "val_metadata = load_metadata(val_dir, val_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_name, annotation_info in mAnnotated_json.items():\n",
    "    for train_frame in train_metadata:\n",
    "        if frame_name in train_frame['image_path']:\n",
    "            prepare_train_val(train_dir, annotations_dir, frame_name, annotation_info)\n",
    "            \n",
    "    \n",
    "    for val_frame in val_metadata:\n",
    "        if frame_name in val_frame['image_path']:\n",
    "            prepare_train_val(val_dir, annotations_dir, frame_name, annotation_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy complete for the split annotations_metadata.json with images - True\n"
     ]
    }
   ],
   "source": [
    "predict_metadata = load_metadata(activeLearning_dir, \"predict.json\")\n",
    "predict_metadata\n",
    "annotation_metadata = load_metadata(activeLearning_dir, \"annotations_metadataAL.json\")\n",
    "annotation_metadata.extend(predict_metadata)\n",
    "split_data_combine = (\n",
    "        ('annotations', 'annotations_metadata.json'),\n",
    "        (annotation_metadata, True),\n",
    "        (source_dir, activeLearning_dir)\n",
    "    )\n",
    "\n",
    "copyframes(split_data_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dir = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/stratifySplit_frames/activeLearning/predict\"\n",
    "\n",
    "combinedAnnotated_json = load_metadata(annotations_dir, annotation_json)\n",
    "for label in os.listdir(predict_dir):\n",
    "    if label.endswith(\".txt\"):\n",
    "        label_path = os.path.join(predict_dir, label)\n",
    "\n",
    "        image_name = label.replace('txt', 'jpg')\n",
    "        img_path = os.path.join(predict_dir, image_name)\n",
    "        img_w, img_h = get_image_size(img_path)\n",
    "        \n",
    "        predictions = yolo_txt_to_annotation_json(label_path, image_name,img_w, img_h, [\"nose\", \"earL\", \"earR\", \"tailB\"])\n",
    "        combinedAnnotated_json.update(predictions)\n",
    "\n",
    "save_metadata(annotations_dir, \"annotation.json\", combinedAnnotated_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BehavTrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
