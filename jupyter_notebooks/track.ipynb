{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "from ffprobe import FFProbe\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CONFIGURATION / HYPERPARAMETERS =======================\n",
    "\n",
    "# Maximum allowable distance or mismatch for a valid match.\n",
    "# Typically, you'd define a function that returns a \"cost\" or \"distance\",\n",
    "# and if cost > COST_THRESHOLD, you treat it as an invalid match.\n",
    "COST_THRESHOLD = 5\n",
    "\n",
    "# If a track is unmatched for this many consecutive frames, we delete it.\n",
    "MAX_UNMATCHED_FRAMES = 100\n",
    "\n",
    "# If the detection and track have a cost above this threshold, set the cost to large.\n",
    "LARGE_COST = 1e9\n",
    "\n",
    "# Next ID for new tracks\n",
    "TRACK_ID_START = 1\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_bbox(b1, b2):\n",
    "    \"\"\"\n",
    "    Compute IoU of two bounding boxes in (x1, y1, x2, y2) format.\n",
    "      b1, b2 = (x1, y1, x2, y2) in the same coordinate system.\n",
    "    \"\"\"\n",
    "    # Intersection\n",
    "    ix1 = max(b1[0], b2[0])\n",
    "    iy1 = max(b1[1], b2[1])\n",
    "    ix2 = min(b1[2], b2[2])\n",
    "    iy2 = min(b1[3], b2[3])\n",
    "\n",
    "    iw = max(0., ix2 - ix1)\n",
    "    ih = max(0., iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "\n",
    "    # Union\n",
    "    area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
    "    area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    if union < 1e-9:\n",
    "        return 0.\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_distance(b1, b2):\n",
    "    \"\"\"\n",
    "    Euclidean distance between centers of two bounding boxes\n",
    "    in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    cx1 = 0.5*(b1[0] + b1[2])\n",
    "    cy1 = 0.5*(b1[1] + b1[3])\n",
    "    cx2 = 0.5*(b2[0] + b2[2])\n",
    "    cy2 = 0.5*(b2[1] + b2[3])\n",
    "    return np.hypot(cx1 - cx2, cy1 - cy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_xyxy(b):\n",
    "    \"\"\"\n",
    "    Convert bounding box from (x_center, y_center, w, h) normalized or pixel\n",
    "    to corner format (x1, y1, x2, y2). You can adapt for your coordinate system.\n",
    "    \"\"\"\n",
    "    x_c, y_c, w, h = b\n",
    "    x1 = x_c - w/2\n",
    "    y1 = y_c - h/2\n",
    "    x2 = x_c + w/2\n",
    "    y2 = y_c + h/2\n",
    "    return (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MouseKalmanFilter:\n",
    "    \"\"\"\n",
    "    A KalmanFilter wrapper for bounding box [x_center, y_center, width, height].\n",
    "    You can refine this for better motion modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, init_bbox, init_frame=0):\n",
    "        # init_bbox: (x_center, y_center, w, h)\n",
    "        # We'll track [x, y, s, r] with s ~ scale, r ~ aspect ratio (some standard approach).\n",
    "        # Or you can do a simpler approach [x, y, w, h] directly.\n",
    "        # This example is loosely adapted from e.g. SORT/AB3DMOT style filters.\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "        # State x = [x, y, s, r, vx, vy, vs]\n",
    "        # z = [x, y, s, r]\n",
    "        dt = 1.\n",
    "        self.kf.F = np.array([\n",
    "            [1, 0, 0, 0, dt, 0,  0],\n",
    "            [0, 1, 0, 0, 0,  dt, 0],\n",
    "            [0, 0, 1, 0, 0,  0,  dt],\n",
    "            [0, 0, 0, 1, 0,  0,  0 ],\n",
    "            [0, 0, 0, 0, 1,  0,  0 ],\n",
    "            [0, 0, 0, 0, 0,  1,  0 ],\n",
    "            [0, 0, 0, 0, 0,  0,  1 ]\n",
    "        ], dtype=float)\n",
    "        self.kf.H = np.array([\n",
    "            [1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0]\n",
    "        ], dtype=float)\n",
    "        \n",
    "        # Process noise and measurement noise are hyperparameters\n",
    "        self.kf.P[4:,4:] *= 1000.  # Large initial uncertainty for velocities\n",
    "        self.kf.P *= 10.\n",
    "        self.kf.R[2:,2:] *= 10.  # Scale, ratio measurement noise\n",
    "\n",
    "        # Initialize\n",
    "        x, y, w, h = init_bbox\n",
    "        s = w*h  # scale ~ area\n",
    "        r = w/float(h+1e-6)  # aspect ratio\n",
    "        self.kf.x[:4] = np.array([x, y, s, r]).reshape(-1,1)\n",
    "\n",
    "        self.update(init_bbox)\n",
    "\n",
    "    def predict(self):\n",
    "        self.kf.predict()\n",
    "        return self.get_bbox()\n",
    "\n",
    "    def update(self, bbox):\n",
    "        # bbox is (x, y, w, h)\n",
    "        # Convert to [x, y, s, r]\n",
    "        x, y, w, h = bbox\n",
    "        s = w*h\n",
    "        r = w/float(h+1e-6)\n",
    "        z = np.array([x, y, s, r])\n",
    "        self.kf.update(z)\n",
    "        return self.get_bbox()\n",
    "\n",
    "    def get_bbox(self):\n",
    "        \"\"\"\n",
    "        Convert [x, y, s, r] in self.kf.x to (x, y, w, h).\n",
    "        \"\"\"\n",
    "        x, y, s, r = self.kf.x[:4].reshape(-1)\n",
    "\n",
    "        if s <= 0:\n",
    "            # clamp s to small epsilon\n",
    "            s = 1e-6\n",
    "            self.kf.x[2] = s\n",
    "            \n",
    "        w = np.sqrt(s*r)\n",
    "        h = np.sqrt(s/r)\n",
    "\n",
    "\n",
    "        return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    \"\"\"\n",
    "    Represents a tracked mouse. Stores KalmanFilter, ID, keypoints, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, detection, track_id):\n",
    "        self.id = track_id\n",
    "        # detection['bbox'] is assumed in (x_center, y_center, w, h)\n",
    "        self.kf = MouseKalmanFilter(detection['bbox'])\n",
    "        self.keypoints = detection.get('keypoints', None)  # store if you want\n",
    "        self.time_since_update = 0\n",
    "        self.hits = 1\n",
    "\n",
    "    def predict(self):\n",
    "        predicted_bbox = self.kf.predict()\n",
    "        self.time_since_update += 1\n",
    "        return predicted_bbox\n",
    "\n",
    "    def update(self, detection):\n",
    "        # detection['bbox'] is (x_center, y_center, w, h)\n",
    "        self.kf.update(detection['bbox'])\n",
    "        self.keypoints = detection.get('keypoints', None)\n",
    "        self.time_since_update = 0\n",
    "        self.hits += 1\n",
    "\n",
    "    def get_bbox_xyxy(self):\n",
    "        \"\"\"\n",
    "        Return bounding box in (x1, y1, x2, y2) for use in cost calculations or display.\n",
    "        \"\"\"\n",
    "        x, y, w, h = self.kf.get_bbox()\n",
    "        return xywh_to_xyxy((x, y, w, h))\n",
    "\n",
    "    def get_bbox_xywh(self):\n",
    "        return self.kf.get_bbox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(track: Track, detection: dict, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Example cost function:\n",
    "      cost = alpha * (1 - iou) + (1-alpha) * center_distance\n",
    "    where 0 <= cost < 2. Lower cost => better match.\n",
    "    \n",
    "    You could incorporate keypoints, color histograms, etc. \n",
    "    \"\"\"\n",
    "    track_xyxy = track.get_bbox_xyxy()\n",
    "    det_xyxy   = xywh_to_xyxy(detection['bbox'])\n",
    "\n",
    "    iou_val   = iou_bbox(track_xyxy, det_xyxy)\n",
    "    cdist_val = center_distance(track_xyxy, det_xyxy)\n",
    "\n",
    "    # Weighted combination\n",
    "    cost = alpha * (1.0 - iou_val) + (1-alpha) * (cdist_val / 100.0)\n",
    "    # The center distance is scaled by e.g. 100 to keep it in a smaller range.\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_detections_to_tracks(tracks, detections, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Build a cost matrix of shape (len(tracks), len(detections)) \n",
    "    using compute_cost, then solve the assignment using Hungarian.\n",
    "    \n",
    "    Returns:\n",
    "      matched_pairs: list of (track_idx, detection_idx)\n",
    "      unmatched_tracks: set of track indices\n",
    "      unmatched_detections: set of detection indices\n",
    "    \"\"\"\n",
    "    if len(tracks) == 0 or len(detections) == 0:\n",
    "        return [], set(range(len(tracks))), set(range(len(detections)))\n",
    "\n",
    "    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float32)\n",
    "    for i, trk in enumerate(tracks):\n",
    "        for j, det in enumerate(detections):\n",
    "            c = compute_cost(trk, det, alpha=alpha)\n",
    "            # If the cost is too large, we can clamp it or ignore it.\n",
    "            if c > COST_THRESHOLD:  \n",
    "                cost_matrix[i, j] = LARGE_COST\n",
    "            else:\n",
    "                cost_matrix[i, j] = c\n",
    "\n",
    "    # print(cost_matrix)\n",
    "    row_idx, col_idx = linear_sum_assignment(cost_matrix)\n",
    "    matched_pairs = []\n",
    "    for r, c in zip(row_idx, col_idx):\n",
    "        # If cost is large, treat as unmatched\n",
    "        if cost_matrix[r, c] >= LARGE_COST:\n",
    "            continue\n",
    "        matched_pairs.append((r, c))\n",
    "\n",
    "    matched_track_indices = set([m[0] for m in matched_pairs])\n",
    "    matched_det_indices   = set([m[1] for m in matched_pairs])\n",
    "\n",
    "    unmatched_tracks = set(range(len(tracks))) - matched_track_indices\n",
    "    unmatched_detections = set(range(len(detections))) - matched_det_indices\n",
    "    return matched_pairs, unmatched_tracks, unmatched_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_multi_mice(all_detections):\n",
    "    \"\"\"\n",
    "    Production-level multi-object tracking logic:\n",
    "      - For each frame, predict track positions\n",
    "      - Build cost matrix + Hungarian assignment\n",
    "      - Update matched tracks, handle unmatched\n",
    "      - Create new tracks for unmatched detections\n",
    "      - Remove 'stale' tracks\n",
    "    Input:\n",
    "      all_detections: dict { frame_idx: [ { 'bbox':(x,y,w,h), 'keypoints':... }, ... ] }\n",
    "    Return:\n",
    "      results_per_frame: dict { frame_idx: { track_id: { 'bbox':..., 'keypoints':... } } }\n",
    "    \"\"\"\n",
    "    frame_indices = sorted(all_detections.keys())\n",
    "    active_tracks = []\n",
    "    next_id = TRACK_ID_START\n",
    "\n",
    "    # We'll store the final bounding boxes for each track per frame.\n",
    "    results_per_frame = {}\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        detections = all_detections[frame_idx]\n",
    "\n",
    "        # 1) PREDICT\n",
    "        for trk in active_tracks:\n",
    "            trk.predict()\n",
    "\n",
    "        # 2) ASSOCIATE\n",
    "        matched_pairs, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(active_tracks, detections)\n",
    "\n",
    "        # 3) UPDATE matched tracks\n",
    "        for (trk_idx, det_idx) in matched_pairs:\n",
    "            active_tracks[trk_idx].update(detections[det_idx])\n",
    "\n",
    "        # 4) For unmatched tracks, just keep them with increased time_since_update\n",
    "        #    (the track.predict() already incremented time_since_update)\n",
    "\n",
    "        # 5) CREATE new tracks for unmatched detections\n",
    "        for ud in unmatched_dets:\n",
    "            new_trk = Track(detections[ud], next_id)\n",
    "            active_tracks.append(new_trk)\n",
    "            next_id += 1\n",
    "\n",
    "        # 6) REMOVE tracks that have been unmatched too long\n",
    "        survived_tracks = []\n",
    "        for trk in active_tracks:\n",
    "            if trk.time_since_update < MAX_UNMATCHED_FRAMES:\n",
    "                survived_tracks.append(trk)\n",
    "        active_tracks = survived_tracks\n",
    "\n",
    "        # 7) Collect results\n",
    "        frame_result = {}\n",
    "        for trk in active_tracks:\n",
    "            bbox = trk.get_bbox_xywh()  # (x_center, y_center, w, h)\n",
    "            frame_result[trk.id] = {\n",
    "                'bbox': bbox,\n",
    "                'keypoints': trk.keypoints\n",
    "            }\n",
    "        results_per_frame[frame_idx] = frame_result\n",
    "\n",
    "    return results_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_resolution(filename):\n",
    "    \"\"\"\n",
    "    Returns (width, height) for the first video stream found in `filename`.\n",
    "    \"\"\"\n",
    "    metadata = FFProbe(filename)\n",
    "    for stream in metadata.streams:\n",
    "        if stream.is_video():\n",
    "            print(dir(stream))\n",
    "            return (int(stream.width), int(stream.height))\n",
    "        \n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPointInBBox(x, y, x1, y1, x2, y2):\n",
    "  return (\n",
    "    x >= x1 and x <= x2 and\n",
    "    y >= y1 and y <= y2\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_txt_to_detection(\n",
    "    txt_path, \n",
    "    frame_index,  \n",
    "    image_width, \n",
    "    image_height,\n",
    "    mAnnotated_flag,\n",
    "    visiblePercentage,\n",
    "    keypoint_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a YOLO-like .txt (with bbox + 4 keypoints in normalized coords),\n",
    "    and returns a dictionary in the original annotation style:\n",
    "\n",
    "    {\n",
    "      \"image_filename\": [\n",
    "        {\n",
    "          \"bbox\": {\"x1\":..., \"y1\":..., \"x2\":..., \"y2\":...},\n",
    "          \"keypoints\": {\n",
    "            \"nose\":  [...],\n",
    "            \"earL\":  [...],\n",
    "            \"earR\":  [...],\n",
    "            \"tailB\": [...]\n",
    "          }\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    if keypoint_names is None:\n",
    "        # You can change the order or number of keypoints as needed:\n",
    "        keypoint_names = [\"nose\", \"earL\", \"earR\", \"tailB\"]\n",
    "\n",
    "    annotations = {frame_index: []}\n",
    "\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        tokens = line.split()\n",
    "        # The first 5 tokens are class_id, x_center, y_center, w, h\n",
    "        class_id    = int(tokens[0])\n",
    "        x_center_n  = float(tokens[1])\n",
    "        y_center_n  = float(tokens[2])\n",
    "        w_n         = float(tokens[3])\n",
    "        h_n         = float(tokens[4])\n",
    "\n",
    "        # Denormalize bounding box\n",
    "        x_center = x_center_n * image_width\n",
    "        y_center = y_center_n * image_height\n",
    "        w        = w_n * image_width\n",
    "        h        = h_n * image_height\n",
    "\n",
    "        x1 = x_center - w / 2\n",
    "        y1 = y_center - h / 2\n",
    "        x2 = x_center + w / 2\n",
    "        y2 = y_center + h / 2\n",
    "\n",
    "        if (x1 == x2 or y1 == y2):\n",
    "            continue\n",
    "\n",
    "        # Next tokens: each keypoint has x_kpt_n, y_kpt_n, v_kpt\n",
    "        # For 4 keypoints, that's 12 tokens, starting at index = 5\n",
    "        keypoints_dict = {}\n",
    "        num_kpts = len(keypoint_names)\n",
    "        \n",
    "        # i.e. for 4 keypoints, range(4) => 0..3\n",
    "        for i in range(num_kpts):\n",
    "            x_kpt_n = float(tokens[5 + 3*i])\n",
    "            y_kpt_n = float(tokens[5 + 3*i + 1])\n",
    "            v_kpt   = float(tokens[5 + 3*i + 2])\n",
    "\n",
    "            # denormalize\n",
    "            x_kpt = x_kpt_n * image_width\n",
    "            y_kpt = y_kpt_n * image_height\n",
    "\n",
    "            if not(isPointInBBox(x_kpt, y_kpt, x1, y1, x2, y2)):\n",
    "                continue\n",
    "            \n",
    "            kpt_name = keypoint_names[i]\n",
    "            \n",
    "            keypoints_dict[kpt_name] = [int(x_kpt), int(y_kpt), 2 if v_kpt > visiblePercentage else 1]\n",
    "\n",
    "        annotations[frame_index].append({\n",
    "            \"bbox\": (x_center, y_center, w, h),\n",
    "            \"bbox_xY\": {\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2\n",
    "            },\n",
    "            \"keypoints\": keypoints_dict\n",
    "            # \"mAnnotated\": mAnnotated_flag\n",
    "        })\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_annotations_on_video(input_video, annotations, color_box, color_kpt, output_video=\"output.mp4\", discard=(False, [])):\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "\n",
    "    # Retrieve video properties\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # or 'XVID'/'avc1' etc.\n",
    "    out    = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_index = 1  # or 0, depending on how your annotations are keyed\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # no more frames in video\n",
    "        \n",
    "        if frame_index in annotations:\n",
    "            # Get all mice info for this frame\n",
    "            for mouse_id, mouse_data in annotations[frame_index].items():\n",
    "\n",
    "                if discard[0] and (mouse_id in discard[1]):\n",
    "                    continue\n",
    "                \n",
    "                # Extract bounding box\n",
    "                bbox = mouse_data['bbox']\n",
    "                x1, y1 = int(bbox['x1']), int(bbox['y1'])\n",
    "                x2, y2 = int(bbox['x2']), int(bbox['y2'])\n",
    "\n",
    "                # Draw the bounding box\n",
    "                # color_box = (0, 255, 255)  # e.g. yellow\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color_box[mouse_id], 2)\n",
    "\n",
    "                # (Optional) Label the mouse ID\n",
    "                cv2.putText(frame, f\"Mouse {mouse_id}\", (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_box[mouse_id], 2)\n",
    "\n",
    "                # Draw each keypoint\n",
    "                keypoints = mouse_data['keypoints']\n",
    "                for kpt_name, (kx, ky, conf) in keypoints.items():\n",
    "                    # conf is a confidence score you can use if needed\n",
    "                    kx, ky = int(kx), int(ky)\n",
    "                    # color_kpt = (0, 255, 0)  # e.g. green\n",
    "                    cv2.circle(frame, (kx, ky), 4, color_kpt[kpt_name], -1)\n",
    "\n",
    "                    # (Optional) label the keypoint name\n",
    "                    cv2.putText(frame, kpt_name, (kx+5, ky),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_kpt[kpt_name], 1)\n",
    "\n",
    "        # Write the modified frame to output video\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"Finished writing annotated video:\", output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(source_dir, metadata_filename):\n",
    "    metadata_filePath = os.path.join(source_dir, metadata_filename)\n",
    "\n",
    "    with open(metadata_filePath, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(output_dir, metadata_filename, metadata):\n",
    "    metadata_outFilePath = os.path.join(output_dir, metadata_filename)\n",
    "\n",
    "    with open(metadata_outFilePath, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_dir = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/vids/results/test1_noTrack/labels\"\n",
    "video_predcitionOn_path = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/vids/processed/Gabbia2-D6-eCig(1)-pre.mp4\"\n",
    "img_w, img_h = get_video_resolution(video_predcitionOn_path)\n",
    "\n",
    "# dict { frame_idx: [ { 'bbox':(x,y,w,h), 'keypoints':... }, ... ] }\n",
    "detections = {}\n",
    "mAnnotated_flag = False\n",
    "visiblePercentage = 0.90\n",
    "for predicted_label in os.listdir(predicted_labels_dir):\n",
    "    if predicted_label.endswith('.txt'):\n",
    "        txt_path = os.path.join(predicted_labels_dir, predicted_label)\n",
    "\n",
    "        temp_holder = predicted_label.split('_')\n",
    "        frame_index = int(temp_holder[1].split('.')[0])\n",
    "\n",
    "        detection = yolo_txt_to_detection(txt_path, frame_index, img_w, img_h, mAnnotated_flag, visiblePercentage, [\"nose\", \"earL\", \"earR\", \"tailB\"])\n",
    "        detections.update(detection)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_detections = track_multi_mice(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_more = 0\n",
    "count_less = 0\n",
    "for i in range(len(detections)):\n",
    "    if len(detections[i+1]) !=  len(tracked_detections[i+1]):\n",
    "        if len(detections[i+1]) <  len(tracked_detections[i+1]):\n",
    "            count_more += 1\n",
    "        else:\n",
    "            count_less += 1\n",
    "\n",
    "print(f\"Number of time tracked detection are more {count_more} and number of time tracked detections are less than the original detection {count_less}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_detections = {}\n",
    "\n",
    "for frame_index, mice in detections.items():\n",
    "    id_mice = {}\n",
    "    \n",
    "    for mouse in mice:\n",
    "        for tracked_mice_id, tracked_mice in tracked_detections[frame_index].items():\n",
    "            if mouse['keypoints'] == tracked_mice['keypoints']:\n",
    "                id_mice[tracked_mice_id] = {'bbox': mouse['bbox_xY'], 'keypoints' : mouse['keypoints']}\n",
    "                # print(mouse['keypoints'], tracked_mice_id, tracked_mice['keypoints'])\n",
    "        \n",
    "    final_detections[frame_index] = id_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(detections)):\n",
    "    if len(detections[i+1]) !=  len(final_detections[i+1]):\n",
    "        count += 1\n",
    "        # print(i+1, len(detections[i+1]), len(tracked_detections[i+1]))\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/mnt/c/Users/karti/chest/CNR/projects/data/neurocig/vids/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metadata(output_path, 'tracked_annotations.json', final_detections)\n",
    "final_detections = load_metadata(output_path, 'tracked_annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "\n",
    "FinalVideo_path = os.path.join(output_path, 'tracked_video.mp4')\n",
    "\n",
    "color_box = {\n",
    "    1 : (0, 255, 255),\n",
    "    2 : (0, 255, 128),\n",
    "    3 : (153, 51, 155),\n",
    "    4 : (255, 255, 0),\n",
    "    5 : (255, 0, 255)\n",
    "}\n",
    "\n",
    "color_kpt = {\n",
    "    'nose' : (153, 204, 255),\n",
    "    'earL' : (255, 182, 78),\n",
    "    'earR' : (255, 102, 102),\n",
    "    'tailB' : (255, 153, 204)\n",
    "}\n",
    "\n",
    "discard = (False, [])\n",
    "\n",
    "overlay_annotations_on_video(video_predcitionOn_path, final_detections, color_box, color_kpt, FinalVideo_path, discard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BehavTrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
